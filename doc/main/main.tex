\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{booktabs} %@{}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage[per-mode=symbol,detect-all]{siunitx}
\usepackage{hyperref}
\usepackage{cleveref} %\Cref{} vs. \cref{}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{mathabx} % for \bigtimes


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}


\title{\LARGE \textbf{Planning Under Uncertainty with Sensor Occlusions in \\ Urban Driving Scenarios} % \\
%\thanks{Ross Alexander is supported by a Stanford Graduate Fellowship (SGF) in Science and Engineering.}
}


\author{\IEEEauthorblockN{  Ross Alexander}
\IEEEauthorblockA{\textit{  Department of Aeronautics and Astronautics} \\
\textit{                    Stanford University} \\
                            Stanford, CA 94305 \\
                            rbalexan@stanford.edu}} % or ORCID


\maketitle

\begin{abstract}
    Safe and efficient autonomous driving in urban scenarios requires a policy that is responsive to uncertainty in the environment and sensors. Partial observability of the environment due to sensor occlusions has led to conservative manually-designed collision avoidance policies that are inefficient. Since collision avoidance is a sequential decision-making problem under uncertainty, it can be posed as a partially observable Markov decision process (POMDP), which can be solved to generate an optimal policy. We leverage the POMDP formulation to generate approximately optimal collision avoidance policies and evaluate the safety and efficiency of the resulting driving behaviors. We find that the policies outperform baseline policies and are generally highly-performant and robust to uncertainty in the environment and sensors.
\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

% Overview of topic sentences
Autonomous driving in urban scenarios requires navigating safely and efficiently in light of significant uncertainty presented by occlusions. In dense urban settings, buildings, signs, cars, and other physical obstacles frequently occlude other road users from the field of view of the sensors. Planning in these occluded scenarios requires judicious safety thresholds in order to ensure collision-free driving. In this paper, we focus on developing safe and efficient policies for collision avoidance while passing a crosswalk with pedestrians emerging from an occluded region.

% What is the problem?                     
We consider the occluded crosswalk scenario depicted in \Cref{fig:occluded_crosswalk_scenario}. The scenario consists of an autonomous vehicle driving along a roadway and a pedestrian crossing the roadway at a crosswalk. The ego vehicle's sensors are occluded by a static obstacle that prevents the pedestrian from being observed. 

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.95\linewidth]{doc/main/occluded_crosswalk_scenario.png}}
    \caption{The occluded crosswalk scenario. The ego vehicle (in blue) is driving along the roadway and the pedestrian (in gray) is crossing the crosswalk. The static obstacle occludes the ego vehicle's sensors and prevents observation of the pedestrian.}
    \label{fig:occluded_crosswalk_scenario}
\end{figure}

% Why is it interesting and important?
% Why is it hard? Why do naive approaches fail?
Urban driving scenarios are often heavily occluded, which presents significant challenges in achieving fully autonomous driving. Occlusions in urban scenarios can arise as a result of \textit{static obstacles}, such as buildings, signs, and parked cars, but can also arise as a result of \textit{dynamic obstacles}, such as moving pedestrians and moving cars. Above all, the autonomous vehicle must navigate safely and avoid collision with pedestrians. However, the autonomous vehicle must also navigate efficiently and cannot be paralyzed by the significant uncertainty in the environment and sensors. A pedestrian's pose and velocity may be completely unobservable due to a sensor occlusion, or at best, partially observable due to sensor uncertainty. As a result, the occluded crosswalk scenario strongly captures the trade-off between safety and efficiency in autonomous driving. Optimizing this trade-off and generating a driving policy that is safe and robust to uncertainty in the environment and sensors is a critical step in urban autonomous driving.

% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
Planning in occluded scenarios faces two main challenges. First, the planning algorithm must be able to handle partial observability. Planning requires not only reasoning about an unoccluded pedestrian's pose and velocity, which can only be observed with finite accuracy and precision, but also reasoning about a potentially occluded pedestrian's pose and velocity, which cannot be observed. And second, real urban driving scenarios often contain multiple road users. While solving the occluded crosswalk problem with a single road user can be useful in theory, it is hardly useful in practice. Unfortunately, solving the occluded crosswalk problem with multiple road users can be intractable for many planning algorithms. Thus, a solution for planning in occluded scenarios must be capable of simultaneously handling partial observability of multiple road users in a way that maintains safety and efficiency, and finally, in a way that respects real-time decision-making constraints.

% What are the key components of my approach and results? Also include any specific limitations.
In this paper, we consider the problem of avoiding collisions with pedestrians crossing behind an occluded region on the side of the road and attempt to recreate and extend the results in \cite{Bouton2018ScalableDriving}. The approach formulates the multiple road user collision avoidance problem as a partially observable Markov decision process (POMDP) with discrete state, action, and observation spaces. The POMDP is solved using offline techniques to obtain driving policies, which are evaluated in simulation and compared against baseline policies on safety and efficiency metrics. 

% Summary of the major contributions in bullet form, mentioning in which sections they can be found. This material doubles as an outline of the rest of the paper, saving space and eliminating redundancy.
We discuss related work in \Cref{sec:related-work} and describe the partially-observable Markov decision process (POMDP) and solution techniques for generating an approximately optimal policy in \Cref{sec:background}. \Cref{sec:proposed-approach} discusses how the collision avoidance problem can be posed as a POMDP and the solution approaches we take in obtaining an approximately optimal policy. \Cref{sec:experiments,sec:results} covers the experimental setup, analysis of the safety and efficiency criteria, and discussion of the resulting driving policies. Finally, we present our conclusions in \Cref{sec:conclusion} and provide some areas for further research in \Cref{sec:future-work}.


\section{Related Work}
\label{sec:related-work}

% Other planning frameworks
Several planning algorithms ensure collision-free paths when passing occluded regions. If objects emerging from occluded regions have a specified maximum velocity, the autonomous vehicle can follow a maximal velocity profile along its original planned path that ensures a collision-free trajectory \cite{Alami2002OnPlans}. Some approaches rely on the time-to-collision (TTC) metric and extensions of the TTC metric, which have shown to be reliable safety measures not only for vehicle collision avoidance, but also for pedestrian collision avoidance \cite{Minderhoud2001ExtendedAssessment, Volz2019InferringCrosswalks}. Many of these planning framework are designed solely around the use of \textit{autonomous emergency braking} (AEB) systems to perform drastic collision avoidance maneuvers. While AEB system planning frameworks are effective at ensuring essentially collision-free driving, they suffer from uncertainty in the environment and sensors that can create inefficient control policies likely to initiate unnecessary severe rapid braking.

% Modeling as a POMDP - single user
In order to develop driving policies robust to uncertainty in the environment and sensors, others have chosen to model the collision avoidance problem as a partially observable Markov decision process (POMDP). POMDPs have been used to successfully generate autonomous driving policies in occluded scenarios. In particular, several approaches have leveraged POMDPs to develop longitudinal-control policies for unsignalized intersections \cite{Brechtel2014ProbabilisticPOMDPs, Hubmann2019AScenarios} and for unsignalized crosswalks \cite{Thornton2018ValuePlanning, Thornton2019TowardValues, Kapania2019ACrosswalks}. Most of these approaches considered only one road user due to the increased algorithmic and computational complexity of considering multiple road users. While these approaches generated progress, their applicability is still limited since most urban scenarios contain multiple road users which limits their practical use.

% Modeling as a POMDP - multiple users
A scalable approach to avoiding multiple road users in both the unsignalized intersection and unsignalized crosswalk was achieved through utility fusion \cite{Bouton2018ScalableDriving}. The multiple road user collision avoidance problem was decomposed into single road user collision avoidance sub-problems enabling solution of the problem that scales linearly with the number of road users considered. The set of single road user optimal belief action utilities were synthesized to generate an approximation to the optimal belief action utility for all road users using a fusion function. The authors consider both sum and minimum fusion functions and find the minimum fusion function generates a more conservative policy. Others have expanded on the success of scalable longitudinal-control policies and developed scalable in-lane coupled longitudinal- and lateral-control policies augmented with AEB systems that maintain safety and improve scenario crossing speeds \cite{Schratter2019PedestrianOcclusions}.

% expanding to multiple road users
%-- the so-called "full-stop."


\section{Background}
\label{sec:background}

A principled and general framework for planning under uncertainty is the partially-observable Markov decision process (POMDP). A POMDP is defined by the tuple $\langle \mathcal{S},\mathcal{A}, \mathcal{O}, T, R, O, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{O}$ is the observation space, $T$ is the transition model, $R$ is the reward model, $O$ is the observation model, and $\gamma$ is the discount factor. A POMDP models the process where an agent in state $s \in \mathcal{S}$ takes action $a \in \mathcal{A}$ and transitions to a state $s' \in \mathcal{S}$ with probability $T(s, a, s') = \textnormal{Pr}(s' \mid s, a)$. Then, the agent observes $o \in \mathcal{O}$ with probability $O(o, s',a) = \textnormal{Pr}(o\mid s', a)$ and receives a real-valued reward $r = R(s, a)$. Rewards in the future are discounted by a factor $\gamma \in (0,1)$, so that immediate rewards are valued more than future rewards.

% belief representation, % belief update % belief is a c
In a POMDP, the state of the environment may not be fully observable, so the agent maintains a belief $b$ about the underlying state. A belief is a categorical probability distribution over the state space that represents the probability of each state being the true underlying state. The belief $b$ is updated after taking action $a$ and observing $o$ using the Bayesian belief update:
\begin{equation}
    b'(s')~\propto~O(o \mid s', a) \sum_{s \in S} T(s' \mid s, a) b(s)
\end{equation}

% intractability and citation
% online and offline methods
% QMDP, SARSOP, and other solvers + brief descriptions
The solution to a POMDP is an optimal policy $\pi^*$ that maps beliefs to actions. Following the optimal policy from any initial state maximizes the expected discounted sum of future rewards. % value function U*(b,a)

\section{Proposed Approach}
\label{sec:proposed-approach}

\subsection{POMDP Model}

\subsubsection{State space} For the occluded crosswalk scenario, we track the poses and velocities of the ego vehicle and the pedestrian in the Fren\'et frame. The ego vehicle state tuple consists of the position along the lane and the velocity in the lane $(s_{ego}, v_{ego})$. Similarly for the pedestrian, the state tuple consists of the position along the crosswalk and the velocity in the crosswalk $(s_{ped}, v_{ped})$. We assume in general that the crosswalk is orthogonal to the roadway.

\subsubsection{Action space} The ego vehicle's actions consist of the set of accelerations $\mathcal{A} = \{-2~\si{\meter\per\square\second}, -1~\si{\meter\per\square\second}, 0~\si{\meter\per\square\second}, 1~\si{\meter\per\square\second}\}$. The accelerations are designed to map to standard strategic maneuvers -- rapid deceleration, deceleration, continuation, and acceleration, respectively.

\subsubsection{Observation space} The observation space is similar to the state space. The ego vehicle's state is assumed to be perfectly observable. If the pedestrian is occluded, the pedestrian's state is unobserved ($s_{absent}, v_{absent}$). If the pedestrian is not occluded, the pedestrian's state is partially observable with the position and velocity following Gaussian distributions centered on the true value. \footnote{with what params} $o_{s_{ped}} \sim \mathcal{N}(o_{s_{ped}} \mid \mu_s, \sigma_s)$ and $o_{v_{ped}} \sim \mathcal{N}(o_{v_{ped}} \mid \mu_v, \sigma_v)$.

\subsubsection{Transition model} The evolution of the ego vehicle is given by a simple constant acceleration model that follows:
\begin{align}
    a_t &= a_{ego} \nonumber \\
    v_t &= v_{t-1} + a_t \Delta t \\
    s_t &= s_{t-1} + v_t \Delta t + \tfrac{1}{2} a_t \Delta t^2 \nonumber
\end{align}
The pedestrian evolves using a simple constant velocity model where the velocity at the following time step is sampled according to a uniform distribution over a set of velocities $V = \{-1~\si{\meter\per\second}, 0~\si{\meter\per\second}, 1~\si{\meter\per\second}\}$ $v \sim \mathcal{U}(v \mid v \in V)$
\begin{align}
    v_t &= v \nonumber \\
    s_t &= s_{t-1} + v_t \Delta t 
\end{align}

\subsubsection{Reward model} Rewards are discounted using a discount factor $\gamma = 0.9$, which encourages the agent to seek immediate rewards over future rewards.  



\subsection{POMDP Solvers}

We use the following two offline methods to generate approximately optimal policies

\subsubsection{QMDP}
QMDP solve the POMDP by assuming full observability at the following time step. \cite{Littman1995LearningUp}

full observability at the next time step

\subsubsection{SARSOP}
Successive Approximation of the Reachable Space under Optimal Policies 
\cite{Kurniawati2009SARSOP:Spaces}

 % number of total states after discretization

\section{Experiments}
\label{sec:experiments}
We formulate the POMDP using \texttt{AutomotivePOMDPs.jl}\footnote{\texttt{https://github.com/sisl/AutomotivePOMDPs.jl}}, which is built on \texttt{POMDPs.jl} \cite{Egorov2017POMDPs.jl:Uncertainty}.
Experiments

action cost = 0
collision cost = -10
gamma = 0.9
pedestrian velocity noise = 0.0

\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.95\linewidth]{sr}}}
    \caption{}
    \label{fig:qmdp}
\end{figure}

\section{Results}
\label{sec:results}


Results and Discussion

\section{Conclusion}
\label{sec:conclusion}

%In general a short summarizing paragraph will do, and under no circumstances should the paragraph simply repeat material from the Abstract or Introduction. In some cases it's possible to now make the original claims more concrete, e.g., by referring to quantitative performance results.

\section{Future Work}
\label{sec:future-work}

The policies discussed in this paper were all generated using offline methods. A future research direction is to explore online methods while avoiding discretization of state, action, and observation spaces. A first step would be to use the partially observable Monte Carlo planning (POMCP) algorithm \cite{Silver2010Monte-CarloPOMDPs} or the determinized sparse partially observable trees (DESPOT) algorithm augmented with importance sampling to include rare, but critical events \cite{Ye2017DESPOT:Regularization, Luo2019ImportanceUncertainty}. 

Beyond this the POMDP could be considered in its natural continuous form and solved with a variety of algorithms based on progressive widening (PW), namely, the partially observable Monte Carlo planning with observation widening (POMCPOW) algorithm \cite{Sunberg2018OnlineSpaces}. We expect these methods to be much more difficult to compute but possibly more efficient given the continuous nature of the state, action, and observation spaces.

It would also be interesting to see the performance of many of these policies in the presence of cyclists, who are important, but frequently neglected road users. Since cyclists typically have much higher velocities, we expect the policies learned from pedestrian behavior to fail, but future research could be done to develop hybrid policies that at least avoid a subset of cyclist collisions while maintaining or improving pedestrian safety.

\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}

%\begin{table}[htbp]
%    \caption{Table Type Styles}
%    \begin{center}
%        \begin{tabular}{|c|c|c|c|}
%            \hline
%            \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%            \cline{2-4} 
%            \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
%            \hline
%            copy& More table copy$^{\mathrm{a}}$& &  \\
%            \hline
%            \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%        \end{tabular}
%        \label{tab1}
%    \end{center}
%\end{table}